{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "\n",
    "#output all items, not just last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "    device= \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"  \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Articles dataset class for easy sampling, iteration, and weight creating\n",
    "class Articles(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        super().__init__()\n",
    "        with open(json_file, \"r\") as data_file:\n",
    "            self.examples = json.loads(data_file.read())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def tokenize(self):\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = re.findall('[\\w]+', self.examples[idx]['text'].lower())\n",
    "\n",
    "    def create_positive_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] == target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def create_negative_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] != target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def map_items(self, word_to_id, url_to_id, publication_to_id, filter=False, min_length=0):\n",
    "        min_length_articles = []\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = [word_to_id.get(word, len(word_to_id)) for word in example['text']]\n",
    "            self.examples[idx]['text'] = [word for word in example['text'] if word != len(word_to_id)]\n",
    "            if filter:\n",
    "                if len(self.examples[idx]['text']) > min_length:\n",
    "                    min_length_articles.append(self.examples[idx])\n",
    "            self.examples[idx]['url'] = url_to_id.get(example['url'], url_to_id.get(\"miscellaneous\"))\n",
    "            self.examples[idx]['model_publication'] = publication_to_id.get(example['model_publication'], publication_to_id.get(\"miscellaneous\"))\n",
    "        return min_length_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire and pass a temperature screening. The sites will be cleaned and sanitized before each event. Trump Victory, the president’s joint fundraising committee, will cover the testing costs. The move comes as Trump expresses his desire for the country to reopen even as medical professionals raise concerns about the potential dangers of doing so. The pandemic death toll passed 100,000 this week. Trump is also dead set on holding the GOP’s August national convention in Charlotte, even as North Carolina officials are raising concerns about safety. The Charlotte area has seen an uptick in coronavirus cases in recent days. Trump has also been itching to resume his trademark rallies, his primary method of connecting with supporters and broadcasting his message. The president’s campaign team has used the events to glean data from attendees which is used to turn out his voters. The president has left the confines of the White House over the past few weeks to hold ostensibly official events in swing states like Arizona and Michigan, where polls have shown him trailing presumptive Democratic nominee Joe Biden. The events have sometimes had the feel of a rally, complete with walk-out music. Trump’s advisers want him to be seen as eager to reopen the country while Democrats push for stay-at-home orders that keep the economy shuttered. Earlier this month, the reelection effort released a one-minute advertisement titled “American Comeback” highlighting the president’s desire to reignite the economy. The spot concluded with Trump’s mantra that he will “make America great again.” The president’s previously busy fundraising schedule came to a halt in March. A planned March 12 fundraiser with GOP megadonor Sheldon Adelson was scrapped, as were a pair of events that month to be headlined by first lady Melania Trump. Even with in-person fundraising events slashed, the president has maintained robust fundraising totals thanks to a massive small-donor operation. Trump’s political machine narrowly outraised Biden in April and has a $187 million cash-on-hand lead over Biden and the Democratic Party. Biden has sworn off in-person fundraisers during the pandemic, instead doing online events from his Delaware home. Couples will need to donate $580,600 to attend the Dallas fundraiser. A single attendee to the New Jersey fundraiser will need to give $250,000. The money will go to Trump Victory, a joint fundraising committee of the Trump campaign, Republican National Committee, and state parties.\n"
     ]
    }
   ],
   "source": [
    "sentences = \"The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire and pass a temperature screening. The sites will be cleaned and sanitized before each event. Trump Victory, the president’s joint fundraising committee, will cover the testing costs. The move comes as Trump expresses his desire for the country to reopen even as medical professionals raise concerns about the potential dangers of doing so. The pandemic death toll passed 100,000 this week. Trump is also dead set on holding the GOP’s August national convention in Charlotte, even as North Carolina officials are raising concerns about safety. The Charlotte area has seen an uptick in coronavirus cases in recent days. Trump has also been itching to resume his trademark rallies, his primary method of connecting with supporters and broadcasting his message. The president’s campaign team has used the events to glean data from attendees which is used to turn out his voters. The president has left the confines of the White House over the past few weeks to hold ostensibly official events in swing states like Arizona and Michigan, where polls have shown him trailing presumptive Democratic nominee Joe Biden. The events have sometimes had the feel of a rally, complete with walk-out music. Trump’s advisers want him to be seen as eager to reopen the country while Democrats push for stay-at-home orders that keep the economy shuttered. Earlier this month, the reelection effort released a one-minute advertisement titled “American Comeback” highlighting the president’s desire to reignite the economy. The spot concluded with Trump’s mantra that he will “make America great again.” The president’s previously busy fundraising schedule came to a halt in March. A planned March 12 fundraiser with GOP megadonor Sheldon Adelson was scrapped, as were a pair of events that month to be headlined by first lady Melania Trump. Even with in-person fundraising events slashed, the president has maintained robust fundraising totals thanks to a massive small-donor operation. Trump’s political machine narrowly outraised Biden in April and has a $187 million cash-on-hand lead over Biden and the Democratic Party. Biden has sworn off in-person fundraisers during the pandemic, instead doing online events from his Delaware home. Couples will need to donate $580,600 to attend the Dallas fundraiser. A single attendee to the New Jersey fundraiser will need to give $250,000. The money will go to Trump Victory, a joint fundraising committee of the Trump campaign, Republican National Committee, and state parties.\"\n",
    "print(' Original: ', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['the', 'white', 'house', 'medical', 'unit', 'and', 'secret', 'service', 'will', 'evaluate', 'attendees', 'before', 'they', \"'\", 're', 'admitted', '.', 'they', \"'\", 'll', 'be', 'required', 'to', 'test', 'negative', 'for', 'the', 'virus', 'on', 'the', 'day', 'of', 'the', 'event', ',', 'complete', 'a', 'health', 'question', '##naire', 'and', 'pass', 'a', 'temperature', 'screening', '.', 'the', 'sites', 'will', 'be', 'cleaned', 'and', 'san', '##iti', '##zed', 'before', 'each', 'event', '.', 'trump', 'victory', ',', 'the', 'president', '’', 's', 'joint', 'fundraising', 'committee', ',', 'will', 'cover', 'the', 'testing', 'costs', '.', 'the', 'move', 'comes', 'as', 'trump', 'expresses', 'his', 'desire', 'for', 'the', 'country', 'to', 're', '##open', 'even', 'as', 'medical', 'professionals', 'raise', 'concerns', 'about', 'the', 'potential', 'dangers', 'of', 'doing', 'so', '.', 'the', 'pan', '##de', '##mic', 'death', 'toll', 'passed', '100', ',', '000', 'this', 'week', '.', 'trump', 'is', 'also', 'dead', 'set', 'on', 'holding', 'the', 'go', '##p', '’', 's', 'august', 'national', 'convention', 'in', 'charlotte', ',', 'even', 'as', 'north', 'carolina', 'officials', 'are', 'raising', 'concerns', 'about', 'safety', '.', 'the', 'charlotte', 'area', 'has', 'seen', 'an', 'up', '##tick', 'in', 'corona', '##virus', 'cases', 'in', 'recent', 'days', '.', 'trump', 'has', 'also', 'been', 'it', '##ching', 'to', 'resume', 'his', 'trademark', 'rallies', ',', 'his', 'primary', 'method', 'of', 'connecting', 'with', 'supporters', 'and', 'broadcasting', 'his', 'message', '.', 'the', 'president', '’', 's', 'campaign', 'team', 'has', 'used', 'the', 'events', 'to', 'g', '##lean', 'data', 'from', 'attendees', 'which', 'is', 'used', 'to', 'turn', 'out', 'his', 'voters', '.', 'the', 'president', 'has', 'left', 'the', 'confines', 'of', 'the', 'white', 'house', 'over', 'the', 'past', 'few', 'weeks', 'to', 'hold', 'ostensibly', 'official', 'events', 'in', 'swing', 'states', 'like', 'arizona', 'and', 'michigan', ',', 'where', 'polls', 'have', 'shown', 'him', 'trailing', 'pre', '##sum', '##ptive', 'democratic', 'nominee', 'joe', 'bid', '##en', '.', 'the', 'events', 'have', 'sometimes', 'had', 'the', 'feel', 'of', 'a', 'rally', ',', 'complete', 'with', 'walk', '-', 'out', 'music', '.', 'trump', '’', 's', 'advisers', 'want', 'him', 'to', 'be', 'seen', 'as', 'eager', 'to', 're', '##open', 'the', 'country', 'while', 'democrats', 'push', 'for', 'stay', '-', 'at', '-', 'home', 'orders', 'that', 'keep', 'the', 'economy', 'shutter', '##ed', '.', 'earlier', 'this', 'month', ',', 'the', 'reelection', 'effort', 'released', 'a', 'one', '-', 'minute', 'advertisement', 'titled', '“', 'american', 'comeback', '”', 'highlighting', 'the', 'president', '’', 's', 'desire', 'to', 'reign', '##ite', 'the', 'economy', '.', 'the', 'spot', 'concluded', 'with', 'trump', '’', 's', 'mantra', 'that', 'he', 'will', '“', 'make', 'america', 'great', 'again', '.', '”', 'the', 'president', '’', 's', 'previously', 'busy', 'fundraising', 'schedule', 'came', 'to', 'a', 'halt', 'in', 'march', '.', 'a', 'planned', 'march', '12', 'fundraiser', 'with', 'go', '##p', 'mega', '##don', '##or', 'sheldon', 'ad', '##els', '##on', 'was', 'scrapped', ',', 'as', 'were', 'a', 'pair', 'of', 'events', 'that', 'month', 'to', 'be', 'headlined', 'by', 'first', 'lady', 'mel', '##ania', 'trump', '.', 'even', 'with', 'in', '-', 'person', 'fundraising', 'events', 'slashed', ',', 'the', 'president', 'has', 'maintained', 'robust', 'fundraising', 'totals', 'thanks', 'to', 'a', 'massive', 'small', '-', 'donor', 'operation', '.', 'trump', '’', 's', 'political', 'machine', 'narrowly', 'out', '##rai', '##sed', 'bid', '##en', 'in', 'april', 'and', 'has', 'a', '$', '187', 'million', 'cash', '-', 'on', '-', 'hand', 'lead', 'over', 'bid', '##en', 'and', 'the', 'democratic', 'party', '.', 'bid', '##en', 'has', 'sworn', 'off', 'in', '-', 'person', 'fundraiser', '##s', 'during', 'the', 'pan', '##de', '##mic', ',', 'instead', 'doing', 'online', 'events', 'from', 'his', 'delaware', 'home', '.', 'couples', 'will', 'need', 'to', 'donate', '$', '580', ',', '600', 'to', 'attend', 'the', 'dallas', 'fundraiser', '.', 'a', 'single', 'attend', '##ee', 'to', 'the', 'new', 'jersey', 'fundraiser', 'will', 'need', 'to', 'give', '$', '250', ',', '000', '.', 'the', 'money', 'will', 'go', 'to', 'trump', 'victory', ',', 'a', 'joint', 'fundraising', 'committee', 'of', 'the', 'trump', 'campaign', ',', 'republican', 'national', 'committee', ',', 'and', 'state', 'parties', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[\\w]+', sentences.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Hello. This is just a test sentence. Im not sure why I am still typing text into this area. Crime and humanity. Trying to stop.\n",
      "Tokenized:  ['hello', '.', 'this', 'is', 'just', 'a', 'test', 'sentence', '.', 'im', 'not', 'sure', 'why', 'i', 'am', 'still', 'typing', 'text', 'into', 'this', 'area', '.', 'crime', 'and', 'humanity', '.', 'trying', 'to', 'stop', '.']\n",
      "Token IDs:  [7592, 1012, 2023, 2003, 2074, 1037, 3231, 6251, 1012, 10047, 2025, 2469, 2339, 1045, 2572, 2145, 22868, 3793, 2046, 2023, 2181, 1012, 4126, 1998, 8438, 1012, 2667, 2000, 2644, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Articles(\"../data/final-data/debugdata/train_basic.json\")\n",
    "val_data = Articles(\"../data/final-data/debugdata/eval_basic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_data)):\n",
    "    train_data.examples[idx]['text'] = tokenizer.tokenize(train_data.examples[idx]['text'])\n",
    "    if len(train_data.examples[idx]['text']) > 512:\n",
    "        train_data.examples[idx]['text'] = train_data.examples[idx]['text'][:512]\n",
    "    train_data.examples[idx]['text'] = tokenizer.encode(\n",
    "                        train_data.examples[idx]['text'],           \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 512)\n",
    "    train_data.examples[idx]['model_publication'] = 1 if train_data.examples[idx]['model_publication'] == 'target' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create batches with positive samples in first half and negative examples in second half\n",
    "class BatchSamplerWithNegativeSamples(torch.utils.data.Sampler):\n",
    "    def __init__(self, pos_sampler, neg_sampler, batch_size, items):\n",
    "        self._pos_sampler = pos_sampler\n",
    "        self._neg_sampler = neg_sampler\n",
    "        self._items = items\n",
    "        assert batch_size % 2 == 0, 'Batch size must be divisible by two for negative samples.'\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        batch, neg_batch = [], []\n",
    "        neg_sampler = iter(self._neg_sampler)\n",
    "        for pos_idx in self._pos_sampler:\n",
    "            batch.append(pos_idx)\n",
    "            neg_idx = pos_idx\n",
    "            # keep sampling until we get a true negative sample\n",
    "            while self._items[neg_idx] == self._items[pos_idx]:\n",
    "                try:\n",
    "                    neg_idx = next(neg_sampler)\n",
    "                except StopIteration:\n",
    "                    neg_sampler = iter(self._neg_sampler)\n",
    "                    neg_idx = next(neg_sampler)\n",
    "            neg_batch.append(neg_idx)\n",
    "            if len(batch) == self._batch_size // 2:\n",
    "                batch.extend(neg_batch)\n",
    "                yield batch\n",
    "                batch, neg_batch = [], []\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._pos_sampler) // self._batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to return necessary data for dataloader to pass into model\n",
    "def collate_fn(examples):\n",
    "    words = []\n",
    "    articles = []\n",
    "    labels = []\n",
    "    publications = []\n",
    "    for example in examples:\n",
    "        words.append(example['text'])\n",
    "        articles.append(example['url'])\n",
    "        labels.append(example['model_publication'])\n",
    "        publications.append(example['publication'])\n",
    "    num_words = [len(x) for x in words]\n",
    "    words = np.concatenate(words, axis=0)\n",
    "    word_attributes = torch.tensor(words, dtype=torch.long)\n",
    "    articles = torch.tensor(articles, dtype=torch.long)\n",
    "    num_words.insert(0,0)\n",
    "    num_words.pop(-1)\n",
    "    attribute_offsets = torch.tensor(np.cumsum(num_words), dtype=torch.long)\n",
    "    publications = torch.tensor(publications, dtype=torch.long)\n",
    "    real_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return publications, articles, word_attributes, attribute_offsets, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<00:00, 290581.38B/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 440473133/440473133 [01:37<00:00, 4499883.22B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
