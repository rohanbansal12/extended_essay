{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "\n",
    "#output all items, not just last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "    device= \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"  \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Articles dataset class for easy sampling, iteration, and weight creating\n",
    "class Articles(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        super().__init__()\n",
    "        with open(json_file, \"r\") as data_file:\n",
    "            self.examples = json.loads(data_file.read())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def tokenize(self):\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = re.findall('[\\w]+', self.examples[idx]['text'].lower())\n",
    "\n",
    "    def create_positive_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] == target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def create_negative_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] != target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def map_items(self, word_to_id, url_to_id, publication_to_id, filter=False, min_length=0):\n",
    "        min_length_articles = []\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = [word_to_id.get(word, len(word_to_id)) for word in example['text']]\n",
    "            self.examples[idx]['text'] = [word for word in example['text'] if word != len(word_to_id)]\n",
    "            if filter:\n",
    "                if len(self.examples[idx]['text']) > min_length:\n",
    "                    min_length_articles.append(self.examples[idx])\n",
    "            self.examples[idx]['url'] = url_to_id.get(example['url'], url_to_id.get(\"miscellaneous\"))\n",
    "            self.examples[idx]['model_publication'] = publication_to_id.get(example['model_publication'], publication_to_id.get(\"miscellaneous\"))\n",
    "        return min_length_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire\n"
     ]
    }
   ],
   "source": [
    "sentences = \"The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire\"\n",
    "print(\"Original: \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['the', 'white', 'house', 'medical', 'unit', 'and', 'secret', 'service', 'will', 'evaluate', 'attendees', 'before', 'they', \"'\", 're', 'admitted', '.', 'they', \"'\", 'll', 'be', 'required', 'to', 'test', 'negative', 'for', 'the', 'virus', 'on', 'the', 'day', 'of', 'the', 'event', ',', 'complete', 'a', 'health', 'question', '##naire']\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[\\w]+', sentences.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-177b96b07302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print the sentence mapped to token ids.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Token IDs: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Articles(\"../data/final-data/debugdata/train_basic.json\")\n",
    "val_data = Articles(\"../data/final-data/debugdata/eval_basic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_data)):\n",
    "    train_data.examples[idx]['text'] = tokenizer.tokenize(train_data.examples[idx]['text'])\n",
    "    if len(train_data.examples[idx]['text']) > 512:\n",
    "        train_data.examples[idx]['text'] = train_data.examples[idx]['text'][:512]\n",
    "    train_data.examples[idx]['text'] = tokenizer.encode(\n",
    "                        train_data.examples[idx]['text'],           \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 512)\n",
    "    train_data.examples[idx]['model_publication'] = 1 if train_data.examples[idx]['model_publication'] == 'target' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create batches with positive samples in first half and negative examples in second half\n",
    "class BatchSamplerWithNegativeSamples(torch.utils.data.Sampler):\n",
    "    def __init__(self, pos_sampler, neg_sampler, batch_size, items):\n",
    "        self._pos_sampler = pos_sampler\n",
    "        self._neg_sampler = neg_sampler\n",
    "        self._items = items\n",
    "        assert batch_size % 2 == 0, 'Batch size must be divisible by two for negative samples.'\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        batch, neg_batch = [], []\n",
    "        neg_sampler = iter(self._neg_sampler)\n",
    "        for pos_idx in self._pos_sampler:\n",
    "            batch.append(pos_idx)\n",
    "            neg_idx = pos_idx\n",
    "            # keep sampling until we get a true negative sample\n",
    "            while self._items[neg_idx] == self._items[pos_idx]:\n",
    "                try:\n",
    "                    neg_idx = next(neg_sampler)\n",
    "                except StopIteration:\n",
    "                    neg_sampler = iter(self._neg_sampler)\n",
    "                    neg_idx = next(neg_sampler)\n",
    "            neg_batch.append(neg_idx)\n",
    "            if len(batch) == self._batch_size // 2:\n",
    "                batch.extend(neg_batch)\n",
    "                yield batch\n",
    "                batch, neg_batch = [], []\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._pos_sampler) // self._batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to return necessary data for dataloader to pass into model\n",
    "def collate_fn(examples):\n",
    "    words = []\n",
    "    articles = []\n",
    "    labels = []\n",
    "    publications = []\n",
    "    for example in examples:\n",
    "        words.append(example['text'])\n",
    "        articles.append(example['url'])\n",
    "        labels.append(example['model_publication'])\n",
    "        publications.append(example['publication'])\n",
    "    num_words = [len(x) for x in words]\n",
    "    words = np.concatenate(words, axis=0)\n",
    "    word_attributes = torch.tensor(words, dtype=torch.long)\n",
    "    articles = torch.tensor(articles, dtype=torch.long)\n",
    "    num_words.insert(0,0)\n",
    "    num_words.pop(-1)\n",
    "    attribute_offsets = torch.tensor(np.cumsum(num_words), dtype=torch.long)\n",
    "    publications = torch.tensor(publications, dtype=torch.long)\n",
    "    real_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return publications, articles, word_attributes, attribute_offsets, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "#model = BertForSequenceClassification.from_pretrained(\"../../Data/Bert/Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 1, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"../../Data/Bert/Model\")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bert_emb = model.bert.embeddings.word_embeddings.weight.data.cpu().numpy()\n",
    "np.savetxt(\"/users/rohan/news-classification/data/BERT/u-map/raw_embs.tsv\", bert_emb, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Tokenizer Initialized!\n",
      "0\n",
      "5000\n",
      "10000\n",
      "Test Data Tokenized\n",
      "0\n",
      "5000\n",
      "10000\n",
      "All Data Tokenized!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "import arguments.train_arguments as arguments\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "import sampling.sampler_util as sampler_util\n",
    "import training.eval_util as eval_util\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "test_data = Articles(\"/users/rohan/news-classification/data/final-data/test.json\")\n",
    "eval_data = Articles(\"/users/rohan/news-classification/data/final-data/evaluation.json\")\n",
    "print(\"Data Loaded\")\n",
    "\n",
    "# initialize tokenizer from BERT library\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "print(\"Tokenizer Initialized!\")\n",
    "\n",
    "test_data.tokenize(tokenizer)\n",
    "print(\"Test Data Tokenized\")\n",
    "eval_data.tokenize(tokenizer)\n",
    "print(\"All Data Tokenized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dir = Path(\"/users/rohan/news-classification/data/BERT/dictionaries/\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dictionary_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [test_data, eval_data]:\n",
    "    dataset.map_items(tokenizer,\n",
    "                      final_url_ids,\n",
    "                      final_publication_ids,\n",
    "                      filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/rohan/news-classification/data/BERT/mapped-data/test.json\", \"w\") as file:\n",
    "        json.dump(test_data.examples, file)\n",
    "with open(\"/users/rohan/news-classification/data/BERT/mapped-data/evaluation.json\", \"w\") as file:\n",
    "        json.dump(eval_data.examples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'encode_tokenized',\n",
       " 'encode_tokenized_batch',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalize',\n",
       " 'num_special_tokens_to_add',\n",
       " 'post_process',\n",
       " 'save',\n",
       " 'token_to_id',\n",
       " 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"/users/rohan/news-classification/data/BERT/bert-base-uncased.txt\", lowercase=True\n",
    ")\n",
    "\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, my dog is cute\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, my dog is cute'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 7592, 1010, 2026, 3899, 2003, 10140, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[0], [1], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.squeeze(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
