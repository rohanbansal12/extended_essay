My hypothesis that \acrshort{bert} would outperform \acrshort{rfs} was proved to be incorrect, as \acrshort{rfs} actually performed over $6\%$ better as shown in Table~\ref{tab:recall}. However, my other inference that \acrlong{rfs} would overfit faster was shown to be correct by Figure~\ref{fig:training-recall}.

The results were obviously statistically significant, which can easily be deduced by the breakdown of the datasets (Refer to Appendix~\ref{appendix:data-info})
. Based on this data distribution of the test set, a completely random sorting of the articles would result in an anticaptory recall @ 1000 of $0.15\%$ or roughly 2 true positives. The performance of both models was significantly greater than this baseline, indicating that the models were able to learn intrinsic qualities of the text which allowed for the generation of improved predictions on untrained data. However, the difference between the two models' performance is still ambiguous and could simply be due to random error, something which was indicated by the qualitiatative analysis of the predictions by editors at The Browser. Despite this, even comparable results by \acrshort{rfs} indicate a better result than \acrshort{bert}, due to the drastically reduced training and prediction time and the simplistic mathematical framework.


I was, however, intrigued by my initial hypothesis being incorrect, as \acrshort{bert} should theoretically have outperformed \acrshort{rfs}. There were a few possible explanations. Firstly, machine learning approaches sometimes struggle on certain tasks because of the nature of the dataset, so it is possible that \acrshort{bert} was simply unable to fit the data due to an inherent mathematical hindrance in its workings, although this is unlikely~\parencite{domingos2012few}. It is also possible that \acrshort{bert} overfit the training set in an atypical manner. Because of \acrshort{bert}'s inherent complexity, it can be prone to overfitting a dataset due to the assumption of more complicated relationships between the variables and labels. In contrast, \acrlong{rfs} assumed a simple, linear relationship which requires more data to overfit~\parencite{domingos2012few}. It is also possible that there is no direct causational relationship between the words of an article and its "quality", thus the relationships learned by both models were specific only to the dataset chosen. This is also unlikely, as the performance seen was significantly higher than random, however it is a common issue with machine-learning. Finally, \acrshort{bert} typically requires huge loads of data to effectively train, due to its 100 million+ parameters. There is a chance that the training data was simply not large enough for BERT to effectively fit on~\parencite{gonzlezcarvajal2020comparing}.

For future work, it would be important to introduce regularization into the training to further penalize incorrect predictions. This may hinder \acrshort{rfs}' ability, but could boost \acrshort{bert}. Collecting more data could also provide more insight into whether a lack of examples contributed to the final results.