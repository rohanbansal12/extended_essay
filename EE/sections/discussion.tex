My hypothesis that \acrshort{bert} would outperform \acrshort{rfs} was proved to be incorrect, as \acrshort{rfs} actually performed over $6\%$ better as shown in~\Cref{tab:recall}. However, my other inference that \acrlong{rfs} would overfit faster was shown to be correct by~\Cref{fig:training-recall}.

The results were statistically significant, which can be deduced by the breakdown of the datasets (refer to~\Cref{appendix:data-info}). Based on this data distribution of the test set, a completely random sorting of the articles would result in an anticipatory recall @ 1000 of $0.15\%$or roughly 2 true positives. The performance of both models was significantly greater than this baseline, indicating that the models were able to learn intrinsic qualities of the text which allowed for the generation of improved predictions on untrained data. However, the difference between the two models' performance is still ambiguous and could simply be due to random error, something which was indicated by the qualitative analysis of the predictions by editors at The Browser. Despite this, even comparable results by \acrshort{rfs} indicate a better result than \acrshort{bert}, due to the drastically reduced training time and the simplistic mathematical framework. Because of this simplicity, \acrshort{rfs} can also be deployed for a real-world task extremely cheaply (such as a visual interface or demonstration), while \acrshort{bert} would undoubtedly require a GPU-based, expensive hosting solution.

I was intrigued by my initial hypothesis being incorrect, as \acrshort{bert} should theoretically have outperformed \acrshort{rfs}, but there were some plausible explanations. Firstly, machine learning approaches sometimes struggle on certain tasks because of the nature of the dataset, so it is possible that \acrshort{bert} was unable to fit the data due to an inherent mathematical hindrance, although this is unlikely~\parencite{domingos2012few}. It is also possible that \acrshort{bert} overfit the training set in an atypical manner. Because of \acrshort{bert}'s complexity, it can be prone to overfitting a dataset due to the assumption of more complicated relationships between the variables and labels. In contrast, \acrlong{rfs} assumes a dot-product relationship which requires more data to overfit~\parencite{domingos2012few}. It is also possible that there is no direct causational relationship between the words of an article and its "quality", thus the relationships learned by both models were specific only to the dataset chosen. This is also unlikely, as the performance seen was significantly higher than random, but does merit further analysis. As presented above, there are some linear motives in the analysis of quality-writing, including things such as structure and editor frameworks. This makes it difficult for a researcher or evaluator to fully understand whether the premise of quality is subjective or objective. As stated, the results indicate a strong improvement compared to random performance, which theoretically suggests there are mathematical relationships that can be used to infer the quality of written content, however, it remains difficult to draw a more broad generalization because of the size and nature of the dataset utilized in the study. Finally, \acrshort{bert} requires large amounts of data to effectively train, due to its 100 million+ parameters. There is a chance that the training data was not sufficient for \acrshort{bert} to learn from~\parencite{gonzlezcarvajal2020comparing}.

For future work, it would be important to introduce regularization into the training to further penalize incorrect predictions. This may hinder \acrshort{rfs}' ability, but could boost \acrshort{bert}. Collecting more data could also provide more insight into whether a lack of examples contributed to the final results.