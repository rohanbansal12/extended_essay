My hypothesis that \acrshort{bert} would outperform \acrshort{rfs} was proved to be incorrect, as \acrshort{rfs} actually performed over $6\%$ better as shown in Table~\ref{tab:recall}. However, my other inference that \acrlong{rfs} would overfit faster was shown to be correct by Figure~\ref{fig:training-recall}.

I was intrigued by my initial hypothesis being incorrect, as \acrshort{bert} should theoretically have outperformed \acrshort{rfs}. There were a few possible explanations. Firstly, machine learning approaches sometimes struggle on certain tasks because of the nature of the dataset, so it is possible that \acrshort{bert} was simply unable to fit the data due to an inherent mathematical hindrance in its workings, although this is unlikely~\parencite{domingos2012few}. It is also possible that \acrshort{bert} overfit the training set in an atypical manner. Because of \acrshort{bert}'s inherent complexity, it can be prone to overfitting a dataset due to the assumption of more complicated relationships between the variables and labels. In contrast, \acrlong{rfs} assumed a simple, linear relationship which requires more data to overfit~\parencite{domingos2012few}. It is also possible that there is no direct causational relationship between the words of an article and its "quality", thus the relationships learned by both models were specific only to the dataset chosen. This is also unlikely, as the performance seen was significantly higher than random, however it is a common issue with machine-learning. Finally, \acrshort{bert} typically requires huge loads of data to effectively train, due to its 100 million+ parameters. There is a chance that the training data was simply not large enough for BERT to effectively fit on~\parencite{gonzlezcarvajal2020comparing}.

For future work, it would be important to introduce regularization into the training to further penalize incorrect predictions. This may hinder \acrshort{rfs}' ability, but could boost \acrshort{bert}. Collecting more data could also provide more insight into whether a lack of examples contributed to the final results.