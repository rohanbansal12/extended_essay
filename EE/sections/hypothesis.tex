Based on the information presented, it is apparent that \acrlong{rfs} is more flexible than \acrshort{bert}. \acrshort{bert} utilizes over 100 million parameters and thus requires more computing power and GPU memory for identical tasks. However, this large gap in parameters and empirical evidence suggests that \acrshort{bert} will outperform \gls{rfs} on most NLP tasks due to an ability to capture more complicated relationships~\parencite{devlin2019bert:}.

This experiment will analyze both the time taken to train the models on the same data, but also the recall/performance of the models. Recall will be tested by looking at the percentage of true positives in the top 1000 predictions of each model. I hypothesize that \acrshort{bert} will have a moderate advantage in recall performance, but \acrshort{rfs} will overfit on the data many magnitudes faster.