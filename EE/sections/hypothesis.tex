Based on the information presented about both approaches, it is very clear that \acrlong{rfs} is more flexible than \acrshort{bert}. \acrshort{bert} utilizes over 100 million parameters and thus requires significantly more computing power and GPU memory to be run even on smaller datasets. However, this large gap in parameters and empirical evidence suggests that \acrshort{bert} will outperform \gls{rfs} on practically any NLP task as it is able to better fit unique datasets ~\parencite{devlin2019bert:}.

This experiment will analyze both the time taken to train the models on the same data, but also the recall/performance of the models. Recall will be tested by looking at the percentage of true positives in the top 1000 predictions of each model. I hypothesize that \acrshort{bert} will have a moderate advantage in recall performance, but \acrshort{rfs} will overfit on the data many magnitudes faster.