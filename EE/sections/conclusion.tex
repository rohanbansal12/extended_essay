The purpose of this experiment was to compare two modern machine-learning approaches to NLP in their ability to recommend writing. The theoretical aspects of both models were explored, including the mathematical framework, and gridsearches were run to determine the best-performing models. It is still difficult to draw a direct global conclusion in this type of experiment because of the limited dataset, and the other potential limitations discussed above.

However, when answering the initial research question, \acrlong{rfs} outperformed \acrshort{bert} in recall at 1000 on the test set and generated better results significantly faster. A novel set-based dot-product approach was able to suggest "high-quality" writing at a rate much higher than random. \acrshort{rfs} can also be implemented in an online application or command-line tool to help readers and curators filter large amounts of new articles to the more relevant and interesting reads.