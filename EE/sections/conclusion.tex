The purpose of this experiment was to compare two modern machine-learning approaches to NLP in their ability to recommend writing. The theoretical aspects of both models were explored, including the mathematical framework, and gridsearchs were run to determine the best-performing models. It is difficult to draw a direct conclusion in this type of exeriment because of the limited dataset, and the other potential limitations discussed above.

However, when answering the initial research question, \acrlong{rfs} outperformed \acrshort{bert} in recall at 1000 on the test set and generated better results significantly faster. A novel set-based dot-product approach was able to suggest "high-quality" writing at a rate much higher than random. \acrshort{rfs} can also be implemented in an online application or command-line tool to help readers and curators filter large amounts of new articles to the more relevant and interesting reads.