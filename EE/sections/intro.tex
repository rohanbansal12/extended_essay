This essay is focused on comparing two modern machine learning approaches in their ability to classify and predict “quality” writing. This type of writing can be thought of as the material published in curated newsletters (such as The Browser) or quality focused publications such as Longform.org. There seems to be a general parcity of serious work in this space, partially because it may be quite difficult to train a machine to understand what we humans would consider fantastic literature and also because finding adequate data (especially quality pieces) can be difficult and expensive. The two approaches considered are \gls{rfs}~\parencite{altosaar2020rankfromsets:} and \acrshort{bert}~\parencite{devlin2019bert:}, a transformer based model. The primary method of comparison between the two models will come from their respective recall on the held out evaluation set. The top 1000 predictions for each model will be considered, and the percentage of true positives will determine the model’s score. Other time and computing constraints will also be considered and discussed throughout to provide enhanced context for all results. Hence, the question: \emph{How do transformer models with pre-contextualized word embeddings compare to set-based recommendation models, such as a dot-product model, in their ability to classify articles on the basis of the "quality" of their writing, as assessed by expert humans?}