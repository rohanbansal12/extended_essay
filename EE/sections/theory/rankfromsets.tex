\gls{rfs} was proposed by \textcite{altosaar2020rankfromsets:} and was initially used for the task of recommending meals to users based on their prior data of favorite food combinations. I used a slightly modified version in my research. The recommender system utilizes an unordered set of attributes to represent each individual item. An inner-product architeture parametrizes the classifier:
\begin{equation}
\label{eq:inner-product}
  f\left(q, x_m\right) = \theta_p^\top\left(\frac{1}{|x_m|}\sum_{j\in x_m}
  \beta_j\right) + \frac{1}{|x_m|}\sum_{j\in x_m}
  \gamma_j\, .
\end{equation}

Each aspect of \Cref{eq:inner-product} has an intuitive definition:

$\theta_p^\top$: the publication embedding for publication $p$. In this essay, there is only one 
publication (that of quality writing), however the simplicity of the model allows for easy 
scalability to generate predictions for multiple publications.

$x_m$: the set of unique word tokens in the article. Because the model does not take 
positional relationships of words into account, the ordered set of tokens can be used to   
mitigate the effects of the repetition of frequently used words such as \emph{the, and, a} etc.

$\beta_j$: the latent quality of each word in the set represented by $x_m$. This is a vector representing the word’s “meaning” that is updated during training. These vectors are summed across the set and normalized by word-list length.

$\gamma_j$: the scalar bias of each word in the set represented by $x_m$. These are used to prevent overfitting and account for qualities of a word that were not represented within the embedding vector (such as frequency). These were also summed and normalized.

Normalization of the sums of biases and vector representations is done to prevent article length from playing an outsized role in predictions. The vector sum is then multiplied with the “quality” writing publication embedding, which has identical dimensions. The normalized word biases are added to generate the final output of the function.

The final probability is defined as:
\begin{equation}
\label{eq:sigmoid_rfs}
p(\yqm = 1 \mid q, m) = \sigma\left( f \left (q, x_m\right) \right)\, 
\end{equation}

In \Cref{eq:sigmoid_rfs}, $\sigma$ is the sigmoid function, defined by:
\begin{equation}
\label{eq:sigmoid}
S(x) = \frac{1}{1 + e^{-x}}, 
\end{equation}

$\sigma$ takes an input and and generates a final probablity between $0$ and $1$, which is then rounded to get a positive or negative prediction ~\parencite{svetoslav_2015}. The implementation of the model is presented through a custom subclass of the PyTorch Module ~\parencite{NEURIPS2019_9015} and ~\parencite{altosaar2020rankfromsets:}:

\begin{minted}{python}
class RankFromSets(nn.Module):
    def __init__(self, n_pubs, n_attrs, emb_size):
        super().__init__()
        self.emb_size = emb_size
        self.publication_embeddings = nn.Embedding(n_pubs, emb_size)
        self.publication_bias = nn.Embedding(n_pubs, 1)
        self.attribute_emb_sum = nn.EmbeddingBag(n_attrs,
                                                emb_size, mode="mean")
        self.attribute_bias_sum = nn.EmbeddingBag(n_attrs, 1, mode="mean")

    # this method resets default parameters to normal distribution
    def reset_parameters(self):
        for module in [self.publication_embeddings, self.attribute_emb_sum]:
            scale = 0.07
            nn.init.uniform_(module.weight, -scale, scale)
        for module in [self.publication_bias, self.attribute_bias_sum]:
            nn.init.zeros_(module.weight)

    def forward(self, publications, word_attributes, attribute_offsets):
        # get publication embedding and bias
        publication_emb = self.publication_embeddings(publications)
        publication_bias = self.publication_bias(publications)
        # get sum of attribute embedding vectors
        article_and_attr_emb = self.attribute_emb_sum(
            word_attributes, attribute_offsets
        )
        # get sum of attribute biases
        attr_bias = self.attribute_bias_sum(word_attributes, attribute_offsets)
        # dot-product of pub_emb and attr_emb sum
        inner_prod = (publication_emb * article_and_attr_emb).sum(-1)
        # add biases to result and return final logits
        logits = inner_prod + attr_bias.squeeze() + publication_bias.squeeze()
        return logits
\end{minted}