\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{step}\PYG{p}{,} \PYG{n}{batch} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{cycle}\PYG{p}{(}\PYG{n}{train\PYGZus{}loader}\PYG{p}{)):}
    \PYG{c+c1}{\PYGZsh{} turn to training mode and calculate loss for backpropagation}
    \PYG{n}{torch}\PYG{o}{.}\PYG{n}{enable\PYGZus{}grad}\PYG{p}{()}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{()}
    \PYG{n}{word\PYGZus{}attributes}\PYG{p}{,} \PYG{n}{attention\PYGZus{}masks}\PYG{p}{,} \PYG{n}{word\PYGZus{}subset\PYGZus{}counts}\PYG{p}{,} \PYG{n}{real\PYGZus{}labels} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{word\PYGZus{}attributes} \PYG{o}{=} \PYG{n}{word\PYGZus{}attributes}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{device}\PYG{p}{)}
    \PYG{n}{attention\PYGZus{}masks} \PYG{o}{=} \PYG{n}{attention\PYGZus{}masks}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{device}\PYG{p}{)}
    \PYG{n}{logits} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n}{word\PYGZus{}attributes}\PYG{p}{,} \PYG{n}{attention\PYGZus{}masks}\PYG{p}{)[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{logits} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{logits}\PYG{p}{)}
    \PYG{n}{L} \PYG{o}{=} \PYG{n}{loss}\PYG{p}{(}\PYG{n}{logits}\PYG{p}{,} \PYG{n}{labels}\PYG{p}{)}
    \PYG{n}{L}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{()}
    \PYG{k}{if} \PYG{n}{args}\PYG{o}{.}\PYG{n}{clip\PYGZus{}grad}\PYG{p}{:}
        \PYG{n}{nn}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{clip\PYGZus{}grad\PYGZus{}norm\PYGZus{}}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(),} \PYG{l+m+mf}{1.0}\PYG{p}{)}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{()}
    \PYG{n}{scheduler}\PYG{o}{.}\PYG{n}{step}\PYG{p}{()}
    \PYG{n}{running\PYGZus{}loss} \PYG{o}{+=} \PYG{n}{L}\PYG{p}{\PYGZob{}\PYGZcb{}}\PYG{o}{.}\PYG{n}{item}\PYG{p}{()}
\end{Verbatim}
